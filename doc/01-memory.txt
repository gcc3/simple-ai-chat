
The large language model, or LLM, is static, it means the AI don't have a memory itself.

It doesn't remeber anything!

How a conversion can continue without a memory.

It is actually, everytime your questions is sent to the LLM, the system will also send the chat history.

By default, this system will send 7 rounds of conversion history.

It called memory length.

You can use command colon set memLength along with a number to set it.

Please note, a longer memory length will consume more tokens.
